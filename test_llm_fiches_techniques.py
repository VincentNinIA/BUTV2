#!/usr/bin/env python3
"""
Test de V√©rification LLM - Fiches Techniques
===========================================

Ce script v√©rifie que :
1. Le LLM re√ßoit bien les fiches techniques compl√®tes
2. Il analyse correctement les informations re√ßues
3. Il fait des choix pertinents bas√©s sur les donn√©es
4. Les prompts contiennent toutes les informations n√©cessaires
"""

import os
import sys
from dotenv import load_dotenv
from datetime import datetime
import time # Added missing import for time

# Ajouter le chemin du projet
sys.path.insert(0, os.path.abspath('.'))

def test_fiches_techniques_retrieval():
    """Test 1: V√©rifier que les fiches techniques sont bien r√©cup√©r√©es"""
    print("\nüîç === TEST 1: R√âCUP√âRATION FICHES TECHNIQUES ===")
    
    try:
        from rag.retrieval import _vector_store, _inventory_df
        
        # Test sur un produit connu
        test_product = "CAISSE US SC 400X300X300MM"
        print(f"Test sur : {test_product}")
        
        # Recherche de fiche technique
        print("üîç Recherche fiche technique...")
        product_docs = _vector_store.similarity_search_with_score(
            f"fiche technique {test_product}",
            k=3  # Plus de r√©sultats pour analyse
        )
        
        if product_docs:
            for i, (doc, score) in enumerate(product_docs):
                print(f"\nüìÑ Fiche {i+1} (score: {score:.3f}):")
                print(f"Contenu (200 premiers chars): {doc.page_content[:200]}...")
                
                # V√©rifier si c'est bien une fiche technique
                content_lower = doc.page_content.lower()
                tech_indicators = [
                    'fiche produit', 'caract√©ristiques techniques', 'conception',
                    'type :', 'force :', 'avantages', 'description d√©taill√©e'
                ]
                
                tech_found = [ind for ind in tech_indicators if ind in content_lower]
                print(f"Indicateurs techniques trouv√©s: {tech_found}")
                
                if tech_found:
                    print("‚úÖ Fiche technique valide d√©tect√©e")
                else:
                    print("‚ö†Ô∏è Contenu technique limit√©")
        else:
            print("‚ùå Aucune fiche technique trouv√©e")
            
        return bool(product_docs)
        
    except Exception as e:
        print(f"‚ùå Erreur r√©cup√©ration fiches: {e}")
        return False

def test_llm_prompt_construction():
    """Test 2: V√©rifier la construction des prompts pour le LLM"""
    print("\nü§ñ === TEST 2: CONSTRUCTION PROMPTS LLM ===")
    
    try:
        from rag.core import answer
        from rag.retrieval_optimized import fetch_docs_optimized
        
        # Test avec un cas probl√©matique pour d√©clencher alternatives
        test_product = "CAISSE US SC 400X300X300MM"
        required_qty = 1000
        prix_propose = 0.25  # Prix bas pour d√©clencher probl√®me marge
        
        print(f"Test: {test_product}, Qt√©: {required_qty}, Prix: {prix_propose}‚Ç¨")
        
        # R√©cup√©rer les donn√©es RAG
        print("üì¶ R√©cup√©ration donn√©es RAG...")
        result = fetch_docs_optimized(
            query=f"Analyse {test_product}",
            product_id=test_product,
            required_qty=required_qty,
            prix_propose=prix_propose
        )
        
        if result and result.get('alternatives'):
            produit = result['produit']
            alternatives = result['alternatives']
            
            print(f"\n‚úÖ Donn√©es r√©cup√©r√©es:")
            print(f"   - Produit principal: {produit.get('name', 'N/A')}")
            print(f"   - Alternatives trouv√©es: {len(alternatives)}")
            print(f"   - Marge produit: {produit.get('marge_actuelle', 'N/A')}‚Ç¨")
            
            # Analyser le contenu des alternatives
            print(f"\nüìä Analyse contenu alternatives:")
            for i, alt in enumerate(alternatives[:3]):  # Top 3
                print(f"\n   Alternative {i+1}: {alt.get('name', 'N/A')}")
                print(f"   - Stock: {alt.get('stock_disponible', 'N/A')}")
                print(f"   - Marge: {alt.get('marge', 'N/A')}‚Ç¨")
                print(f"   - Similarit√©: {alt.get('similarite_technique', 'N/A')}")
                
                # V√©rifier les fiches techniques des alternatives
                description = alt.get('description', '')
                if description:
                    print(f"   - Fiche technique: OUI ({len(description)} chars)")
                    if 'fiche produit' in description.lower():
                        print("     ‚úÖ Fiche technique structur√©e d√©tect√©e")
                    else:
                        print(f"     üìÑ Contenu: {description[:100]}...")
                else:
                    print("   - Fiche technique: NON")
            
            return True
        else:
            print("‚ùå Aucune alternative r√©cup√©r√©e")
            return False
            
    except Exception as e:
        print(f"‚ùå Erreur construction prompt: {e}")
        return False

def test_llm_decision_making():
    """Test 3: V√©rifier la qualit√© des d√©cisions du LLM"""
    print("\nüß† === TEST 3: QUALIT√â D√âCISIONS LLM ===")
    
    try:
        from ninia.agent import NiniaAgent
        import os
        
        # Cr√©er l'agent avec la cl√© API
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("‚ùå OPENAI_API_KEY manquante dans l'environnement")
            return False
            
        agent = NiniaAgent(api_key)
        
        # Test cases avec attentes sp√©cifiques
        test_cases = [
            {
                "name": "Cas 1: Marge insuffisante",
                "input": "76000 00420000 CAISSE US SC 400X300X300MM Qt√© 100 Prix : 0,20‚Ç¨",
                "expectation": "Devrait identifier marge insuffisante et proposer alternatives"
            },
            {
                "name": "Cas 2: Stock insuffisant", 
                "input": "76000 00420000 CAISSE US SC 450X300X230MM Qt√© 5000 Prix : 0,7‚Ç¨",
                "expectation": "Devrait identifier rupture stock et proposer alternatives"
            },
            {
                "name": "Cas 3: Produit correct",
                "input": "76000 00420000 CAISSE US SC 200X150X150MM Qt√© 50 Prix : 1,0‚Ç¨",
                "expectation": "Devrait valider le produit et mentionner alternatives"
            }
        ]
        
        results = []
        for test_case in test_cases:
            print(f"\nüéØ {test_case['name']}")
            print(f"Input: {test_case['input']}")
            print(f"Attente: {test_case['expectation']}")
            
            # Analyser avec l'agent
            print("ü§ñ Analyse par l'agent...")
            start_time = time.time()
            
            response = agent.analyser_commande(test_case['input'])
            
            analysis_time = time.time() - start_time
            
            # Analyser la r√©ponse
            response_text = response.get('response', '') if isinstance(response, dict) else str(response)
            
            print(f"‚è±Ô∏è Temps d'analyse: {analysis_time:.2f}s")
            print(f"üìù R√©ponse (200 premiers chars): {response_text[:200]}...")
            
            # V√©rifications qualit√©
            quality_checks = {
                "mentionne_alternatives": "alternative" in response_text.lower(),
                "cite_chiffres": any(char.isdigit() for char in response_text),
                "analyse_marge": "marge" in response_text.lower(),
                "analyse_stock": "stock" in response_text.lower(),
                "propose_solution": any(word in response_text.lower() for word in ["propose", "recommande", "sugg√®re"])
            }
            
            print(f"‚úÖ V√©rifications qualit√©:")
            for check, passed in quality_checks.items():
                status = "‚úÖ" if passed else "‚ùå"
                print(f"   {status} {check.replace('_', ' ').title()}: {passed}")
            
            # Score qualit√©
            quality_score = sum(quality_checks.values()) / len(quality_checks)
            print(f"üìä Score qualit√©: {quality_score:.1%}")
            
            results.append({
                "case": test_case['name'],
                "time": analysis_time,
                "quality_score": quality_score,
                "checks": quality_checks
            })
        
        # Synth√®se
        print(f"\nüìã === SYNTH√àSE QUALIT√â LLM ===")
        avg_time = sum(r['time'] for r in results) / len(results)
        avg_quality = sum(r['quality_score'] for r in results) / len(results)
        
        print(f"‚è±Ô∏è Temps moyen: {avg_time:.2f}s")
        print(f"üìä Qualit√© moyenne: {avg_quality:.1%}")
        
        if avg_quality >= 0.8:
            print("üéâ EXCELLENTE qualit√© des d√©cisions LLM")
        elif avg_quality >= 0.6:
            print("‚úÖ BONNE qualit√© des d√©cisions LLM")
        else:
            print("‚ö†Ô∏è Qualit√© des d√©cisions √Ä AM√âLIORER")
            
        return avg_quality >= 0.6
        
    except Exception as e:
        print(f"‚ùå Erreur test d√©cisions: {e}")
        return False

def test_fiches_alternatives_optimized():
    """Test 4: V√©rifier que la version optimis√©e r√©cup√®re les fiches des alternatives"""
    print("\n‚ö° === TEST 4: FICHES DANS VERSION OPTIMIS√âE ===")
    
    try:
        from rag.retrieval_optimized import fetch_docs_optimized
        from rag.retrieval import _vector_store
        
        # Test avec un cas qui d√©clenche alternatives
        test_product = "CAISSE US SC 400X300X300MM"
        result = fetch_docs_optimized(
            query="Test fiches alternatives",
            product_id=test_product,
            required_qty=1000,
            prix_propose=0.25
        )
        
        if not result or not result.get('alternatives'):
            print("‚ùå Aucune alternative dans version optimis√©e")
            return False
        
        alternatives = result['alternatives']
        print(f"‚úÖ {len(alternatives)} alternatives trouv√©es")
        
        # V√©rifier si les alternatives ont des fiches techniques
        alternatives_with_fiches = 0
        total_fiche_length = 0
        
        for i, alt in enumerate(alternatives):
            alt_name = alt.get('name', 'N/A')
            description = alt.get('description', '')
            
            print(f"\nüìÑ Alternative {i+1}: {alt_name}")
            
            if description:
                alternatives_with_fiches += 1
                total_fiche_length += len(description)
                print(f"   ‚úÖ Fiche pr√©sente ({len(description)} chars)")
                
                # Tester qualit√© de la fiche
                tech_keywords = ['fiche produit', 'caract√©ristiques', 'conception', 'type', 'force']
                found_keywords = [kw for kw in tech_keywords if kw in description.lower()]
                print(f"   üìã Mots-cl√©s techniques: {found_keywords}")
                
            else:
                print(f"   ‚ùå Pas de fiche technique")
                # Essayer de r√©cup√©rer manuellement
                print(f"   üîç Tentative r√©cup√©ration manuelle...")
                try:
                    manual_docs = _vector_store.similarity_search_with_score(
                        f"fiche technique {alt_name}",
                        k=1
                    )
                    if manual_docs:
                        manual_content = manual_docs[0][0].page_content
                        print(f"   üìÑ Fiche trouv√©e manuellement ({len(manual_content)} chars)")
                    else:
                        print(f"   ‚ùå Aucune fiche trouv√©e manuellement")
                except Exception as e:
                    print(f"   ‚ùå Erreur r√©cup√©ration manuelle: {e}")
        
        # Statistiques
        fiche_coverage = alternatives_with_fiches / len(alternatives)
        avg_fiche_length = total_fiche_length / max(alternatives_with_fiches, 1)
        
        print(f"\nüìä Statistiques fiches techniques:")
        print(f"   - Couverture: {fiche_coverage:.1%} ({alternatives_with_fiches}/{len(alternatives)})")
        print(f"   - Taille moyenne: {avg_fiche_length:.0f} caract√®res")
        
        if fiche_coverage >= 0.5:
            print("‚úÖ Couverture acceptable des fiches techniques")
            return True
        else:
            print("‚ö†Ô∏è Couverture insuffisante des fiches techniques")
            return False
        
    except Exception as e:
        print(f"‚ùå Erreur test fiches optimis√©es: {e}")
        return False

def main():
    """Fonction principale de test"""
    load_dotenv()
    
    print("üïê D√©but des tests LLM - Fiches Techniques")
    print(f"üìÖ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Ex√©cuter tous les tests
    tests = [
        ("R√©cup√©ration fiches", test_fiches_techniques_retrieval),
        ("Construction prompts", test_llm_prompt_construction), 
        ("Qualit√© d√©cisions", test_llm_decision_making),
        ("Fiches dans optimis√©", test_fiches_alternatives_optimized)
    ]
    
    results = {}
    for test_name, test_func in tests:
        print(f"\n" + "="*60)
        try:
            success = test_func()
            results[test_name] = success
        except Exception as e:
            print(f"‚ùå Erreur dans {test_name}: {e}")
            results[test_name] = False
    
    # Bilan final
    print(f"\n" + "="*60)
    print(f"üìã === BILAN FINAL ===")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    for test_name, success in results.items():
        status = "‚úÖ R√âUSSI" if success else "‚ùå √âCHEC"
        print(f"   {status} {test_name}")
    
    print(f"\nüéØ Score global: {passed_tests}/{total_tests} ({passed_tests/total_tests:.1%})")
    
    if passed_tests == total_tests:
        print("üéâ TOUS LES TESTS R√âUSSIS - LLM re√ßoit bien les fiches techniques !")
    elif passed_tests >= total_tests * 0.75:
        print("‚úÖ BONNE qualit√© - Quelques am√©liorations possibles")
    else:
        print("‚ö†Ô∏è AM√âLIORATIONS REQUISES - V√©rifier la r√©cup√©ration des fiches")
    
    print(f"\nüèÅ Tests termin√©s: {datetime.now().strftime('%H:%M:%S')}")

if __name__ == "__main__":
    main() 