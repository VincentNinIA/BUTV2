#!/usr/bin/env python3
"""
Test de VÃ©rification LLM - Fiches Techniques
===========================================

Ce script vÃ©rifie que :
1. Le LLM reÃ§oit bien les fiches techniques complÃ¨tes
2. Il analyse correctement les informations reÃ§ues
3. Il fait des choix pertinents basÃ©s sur les donnÃ©es
4. Les prompts contiennent toutes les informations nÃ©cessaires
"""

import os
import sys
from dotenv import load_dotenv
from datetime import datetime
import time # Added missing import for time

# Ajouter le chemin du projet
sys.path.insert(0, os.path.abspath('.'))

def test_fiches_techniques_retrieval():
    """Test 1: VÃ©rifier que les fiches techniques sont bien rÃ©cupÃ©rÃ©es"""
    print("\nğŸ” === TEST 1: RÃ‰CUPÃ‰RATION FICHES TECHNIQUES ===")
    
    try:
        from rag.retrieval import _vector_store, _inventory_df
        
        # Test sur un produit connu
        test_product = "CAISSE US SC 400X300X300MM"
        print(f"Test sur : {test_product}")
        
        # Recherche de fiche technique
        print("ğŸ” Recherche fiche technique...")
        product_docs = _vector_store.similarity_search_with_score(
            f"fiche technique {test_product}",
            k=3  # Plus de rÃ©sultats pour analyse
        )
        
        if product_docs:
            for i, (doc, score) in enumerate(product_docs):
                print(f"\nğŸ“„ Fiche {i+1} (score: {score:.3f}):")
                print(f"Contenu (200 premiers chars): {doc.page_content[:200]}...")
                
                # VÃ©rifier si c'est bien une fiche technique
                content_lower = doc.page_content.lower()
                tech_indicators = [
                    'fiche produit', 'caractÃ©ristiques techniques', 'conception',
                    'type :', 'force :', 'avantages', 'description dÃ©taillÃ©e'
                ]
                
                tech_found = [ind for ind in tech_indicators if ind in content_lower]
                print(f"Indicateurs techniques trouvÃ©s: {tech_found}")
                
                if tech_found:
                    print("âœ… Fiche technique valide dÃ©tectÃ©e")
                else:
                    print("âš ï¸ Contenu technique limitÃ©")
        else:
            print("âŒ Aucune fiche technique trouvÃ©e")
            
        return bool(product_docs)
        
    except Exception as e:
        print(f"âŒ Erreur rÃ©cupÃ©ration fiches: {e}")
        return False

def test_llm_prompt_construction():
    """Test 2: VÃ©rifier la construction des prompts pour le LLM"""
    print("\nğŸ¤– === TEST 2: CONSTRUCTION PROMPTS LLM ===")
    
    try:
        from rag.core import answer
        from rag.retrieval_optimized import fetch_docs_optimized
        
        # Test avec un cas problÃ©matique pour dÃ©clencher alternatives
        test_product = "CAISSE US SC 400X300X300MM"
        required_qty = 1000
        prix_propose = 0.25  # Prix bas pour dÃ©clencher problÃ¨me marge
        
        print(f"Test: {test_product}, QtÃ©: {required_qty}, Prix: {prix_propose}â‚¬")
        
        # RÃ©cupÃ©rer les donnÃ©es RAG
        print("ğŸ“¦ RÃ©cupÃ©ration donnÃ©es RAG...")
        result = fetch_docs_optimized(
            query=f"Analyse {test_product}",
            product_id=test_product,
            required_qty=required_qty,
            prix_propose=prix_propose
        )
        
        if result and result.get('alternatives'):
            produit = result['produit']
            alternatives = result['alternatives']
            
            print(f"\nâœ… DonnÃ©es rÃ©cupÃ©rÃ©es:")
            print(f"   - Produit principal: {produit.get('name', 'N/A')}")
            print(f"   - Alternatives trouvÃ©es: {len(alternatives)}")
            print(f"   - Marge produit: {produit.get('marge_actuelle', 'N/A')}â‚¬")
            
            # Analyser le contenu des alternatives
            print(f"\nğŸ“Š Analyse contenu alternatives:")
            for i, alt in enumerate(alternatives[:3]):  # Top 3
                print(f"\n   Alternative {i+1}: {alt.get('name', 'N/A')}")
                print(f"   - Stock: {alt.get('stock_disponible', 'N/A')}")
                print(f"   - Marge: {alt.get('marge', 'N/A')}â‚¬")
                print(f"   - SimilaritÃ©: {alt.get('similarite_technique', 'N/A')}")
                
                # VÃ©rifier les fiches techniques des alternatives
                description = alt.get('description', '')
                if description:
                    print(f"   - Fiche technique: OUI ({len(description)} chars)")
                    if 'fiche produit' in description.lower():
                        print("     âœ… Fiche technique structurÃ©e dÃ©tectÃ©e")
                    else:
                        print(f"     ğŸ“„ Contenu: {description[:100]}...")
                else:
                    print("   - Fiche technique: NON")
            
            return True
        else:
            print("âŒ Aucune alternative rÃ©cupÃ©rÃ©e")
            return False
            
    except Exception as e:
        print(f"âŒ Erreur construction prompt: {e}")
        return False

def test_llm_decision_making():
    """Test 3: VÃ©rifier la qualitÃ© des dÃ©cisions du LLM"""
    print("\nğŸ§  === TEST 3: QUALITÃ‰ DÃ‰CISIONS LLM ===")
    
    try:
        from ninia.agent import NiniaAgent
        import os
        
        # CrÃ©er l'agent avec la clÃ© API
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            print("âŒ OPENAI_API_KEY manquante dans l'environnement")
            return False
            
        agent = NiniaAgent(api_key)
        
        # Test cases avec attentes spÃ©cifiques
        test_cases = [
            {
                "name": "Cas 1: Marge insuffisante",
                "input": "76000 00420000 CAISSE US SC 400X300X300MM QtÃ© 100 Prix : 0,20â‚¬",
                "expectation": "Devrait identifier marge insuffisante et proposer alternatives"
            },
            {
                "name": "Cas 2: Stock insuffisant", 
                "input": "76000 00420000 CAISSE US SC 450X300X230MM QtÃ© 5000 Prix : 0,7â‚¬",
                "expectation": "Devrait identifier rupture stock et proposer alternatives"
            },
            {
                "name": "Cas 3: Produit correct",
                "input": "76000 00420000 CAISSE US SC 200X150X150MM QtÃ© 50 Prix : 1,0â‚¬",
                "expectation": "Devrait valider le produit et mentionner alternatives"
            }
        ]
        
        results = []
        for test_case in test_cases:
            print(f"\nğŸ¯ {test_case['name']}")
            print(f"Input: {test_case['input']}")
            print(f"Attente: {test_case['expectation']}")
            
            # Analyser avec l'agent
            print("ğŸ¤– Analyse par l'agent...")
            start_time = time.time()
            
            response = agent.analyser_commande(test_case['input'])
            
            analysis_time = time.time() - start_time
            
            # Analyser la rÃ©ponse
            response_text = response.get('response', '') if isinstance(response, dict) else str(response)
            
            print(f"â±ï¸ Temps d'analyse: {analysis_time:.2f}s")
            print(f"ğŸ“ RÃ©ponse (200 premiers chars): {response_text[:200]}...")
            
            # VÃ©rifications qualitÃ©
            quality_checks = {
                "mentionne_alternatives": "alternative" in response_text.lower(),
                "cite_chiffres": any(char.isdigit() for char in response_text),
                "analyse_marge": "marge" in response_text.lower(),
                "analyse_stock": "stock" in response_text.lower(),
                "propose_solution": any(word in response_text.lower() for word in ["propose", "recommande", "suggÃ¨re"])
            }
            
            print(f"âœ… VÃ©rifications qualitÃ©:")
            for check, passed in quality_checks.items():
                status = "âœ…" if passed else "âŒ"
                print(f"   {status} {check.replace('_', ' ').title()}: {passed}")
            
            # Score qualitÃ©
            quality_score = sum(quality_checks.values()) / len(quality_checks)
            print(f"ğŸ“Š Score qualitÃ©: {quality_score:.1%}")
            
            results.append({
                "case": test_case['name'],
                "time": analysis_time,
                "quality_score": quality_score,
                "checks": quality_checks
            })
        
        # SynthÃ¨se
        print(f"\nğŸ“‹ === SYNTHÃˆSE QUALITÃ‰ LLM ===")
        avg_time = sum(r['time'] for r in results) / len(results)
        avg_quality = sum(r['quality_score'] for r in results) / len(results)
        
        print(f"â±ï¸ Temps moyen: {avg_time:.2f}s")
        print(f"ğŸ“Š QualitÃ© moyenne: {avg_quality:.1%}")
        
        if avg_quality >= 0.8:
            print("ğŸ‰ EXCELLENTE qualitÃ© des dÃ©cisions LLM")
        elif avg_quality >= 0.6:
            print("âœ… BONNE qualitÃ© des dÃ©cisions LLM")
        else:
            print("âš ï¸ QualitÃ© des dÃ©cisions Ã€ AMÃ‰LIORER")
            
        return avg_quality >= 0.6
        
    except Exception as e:
        print(f"âŒ Erreur test dÃ©cisions: {e}")
        return False

def test_fiches_alternatives_optimized():
    """Test 4: VÃ©rifier que la version optimisÃ©e rÃ©cupÃ¨re les fiches des alternatives"""
    print("\nâš¡ === TEST 4: FICHES DANS VERSION OPTIMISÃ‰E ===")
    
    try:
        from rag.retrieval_optimized import fetch_docs_optimized
        from rag.retrieval import _vector_store
        
        # Test avec un cas qui dÃ©clenche alternatives
        test_product = "CAISSE US SC 400X300X300MM"
        result = fetch_docs_optimized(
            query="Test fiches alternatives",
            product_id=test_product,
            required_qty=1000,
            prix_propose=0.25
        )
        
        if not result or not result.get('alternatives'):
            print("âŒ Aucune alternative dans version optimisÃ©e")
            return False
        
        alternatives = result['alternatives']
        print(f"âœ… {len(alternatives)} alternatives trouvÃ©es")
        
        # VÃ©rifier si les alternatives ont des fiches techniques
        alternatives_with_fiches = 0
        total_fiche_length = 0
        
        for i, alt in enumerate(alternatives):
            alt_name = alt.get('name', 'N/A')
            description = alt.get('description', '')
            
            print(f"\nğŸ“„ Alternative {i+1}: {alt_name}")
            
            if description:
                alternatives_with_fiches += 1
                total_fiche_length += len(description)
                print(f"   âœ… Fiche prÃ©sente ({len(description)} chars)")
                
                # Tester qualitÃ© de la fiche
                tech_keywords = ['fiche produit', 'caractÃ©ristiques', 'conception', 'type', 'force']
                found_keywords = [kw for kw in tech_keywords if kw in description.lower()]
                print(f"   ğŸ“‹ Mots-clÃ©s techniques: {found_keywords}")
                
            else:
                print(f"   âŒ Pas de fiche technique")
                # Essayer de rÃ©cupÃ©rer manuellement
                print(f"   ğŸ” Tentative rÃ©cupÃ©ration manuelle...")
                try:
                    manual_docs = _vector_store.similarity_search_with_score(
                        f"fiche technique {alt_name}",
                        k=1
                    )
                    if manual_docs:
                        manual_content = manual_docs[0][0].page_content
                        print(f"   ğŸ“„ Fiche trouvÃ©e manuellement ({len(manual_content)} chars)")
                    else:
                        print(f"   âŒ Aucune fiche trouvÃ©e manuellement")
                except Exception as e:
                    print(f"   âŒ Erreur rÃ©cupÃ©ration manuelle: {e}")
        
        # Statistiques
        fiche_coverage = alternatives_with_fiches / len(alternatives)
        avg_fiche_length = total_fiche_length / max(alternatives_with_fiches, 1)
        
        print(f"\nğŸ“Š Statistiques fiches techniques:")
        print(f"   - Couverture: {fiche_coverage:.1%} ({alternatives_with_fiches}/{len(alternatives)})")
        print(f"   - Taille moyenne: {avg_fiche_length:.0f} caractÃ¨res")
        
        if fiche_coverage >= 0.5:
            print("âœ… Couverture acceptable des fiches techniques")
            return True
        else:
            print("âš ï¸ Couverture insuffisante des fiches techniques")
            return False
        
    except Exception as e:
        print(f"âŒ Erreur test fiches optimisÃ©es: {e}")
        return False

def main():
    """Fonction principale de test"""
    load_dotenv()
    
    print("ğŸ• DÃ©but des tests LLM - Fiches Techniques")
    print(f"ğŸ“… {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # ExÃ©cuter tous les tests
    tests = [
        ("RÃ©cupÃ©ration fiches", test_fiches_techniques_retrieval),
        ("Construction prompts", test_llm_prompt_construction), 
        ("QualitÃ© dÃ©cisions", test_llm_decision_making),
        ("Fiches dans optimisÃ©", test_fiches_alternatives_optimized)
    ]
    
    results = {}
    for test_name, test_func in tests:
        print(f"\n" + "="*60)
        try:
            success = test_func()
            results[test_name] = success
        except Exception as e:
            print(f"âŒ Erreur dans {test_name}: {e}")
            results[test_name] = False
    
    # Bilan final
    print(f"\n" + "="*60)
    print(f"ğŸ“‹ === BILAN FINAL ===")
    
    total_tests = len(results)
    passed_tests = sum(results.values())
    
    for test_name, success in results.items():
        status = "âœ… RÃ‰USSI" if success else "âŒ Ã‰CHEC"
        print(f"   {status} {test_name}")
    
    print(f"\nğŸ¯ Score global: {passed_tests}/{total_tests} ({passed_tests/total_tests:.1%})")
    
    if passed_tests == total_tests:
        print("ğŸ‰ TOUS LES TESTS RÃ‰USSIS - LLM reÃ§oit bien les fiches techniques !")
    elif passed_tests >= total_tests * 0.75:
        print("âœ… BONNE qualitÃ© - Quelques amÃ©liorations possibles")
    else:
        print("âš ï¸ AMÃ‰LIORATIONS REQUISES - VÃ©rifier la rÃ©cupÃ©ration des fiches")
    
    print(f"\nğŸ Tests terminÃ©s: {datetime.now().strftime('%H:%M:%S')}")

if __name__ == "__main__":
    main() 